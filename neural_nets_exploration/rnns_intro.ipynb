{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c4b61f-69af-49ae-aebd-4075661ab0d4",
   "metadata": {},
   "source": [
    "# Intro to Recurrent Neural Nets (RNNs)\n",
    "\n",
    "References: \n",
    "* Intro to RNNs: https://victorzhou.com/blog/intro-to-rnns/\n",
    "* Explanation of Entropy: https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Create Virtual Environment: `python3 -m venv datascience-venv`\n",
    "2. Set Virtual Environment: `source datascience-venv/bin/activate`\n",
    "3. Install JupyterLab in your Virtual Env using pip: `pip3 install jupyterlab`\n",
    "4. Install dependencies (`numpy`, `pandas`, `scikit-learn`) into the virtual environment\n",
    "   * `pip3 install pandas`, `pip3 install scikit-learn`\n",
    "5. Add your Virtual Environment as a kernel to Jupyterlab: `python3 -m ipykernel install --user --name=datascience-venv`\n",
    "6. Start JupyterLab from the virtual environment: `jupyter-lab --notebook-dir <location of your notebooks>`\n",
    "7. Make sure your set your Virtual Env's kernel in the notebook that you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a8bb9a-fbcb-4ce1-98e2-4c3d6aff02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from test_data.rnns_testdata import train_data, test_data\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427fa7c-0908-42f9-8ac4-85f7bc47096b",
   "metadata": {},
   "source": [
    "## Training data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2406dffa-f000-465d-a986-9085bdb590ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce all training data into set of unique words\n",
    "# Note: the lambda does x + y - as [1,2,3] + [4,5,6] appends 2 lists together\n",
    "vocabulary = list(set(reduce(\n",
    "    lambda list_elem1, list_elem2: list_elem1+list_elem2,\n",
    "    [key.split(' ') for key in train_data.keys()], \n",
    "    []\n",
    ")))\n",
    "assert len(vocabulary) == 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d36c5d9-836d-4739-b3dd-3b492f9b32f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': True,\n",
       " 'bad': False,\n",
       " 'happy': True,\n",
       " 'sad': False,\n",
       " 'not good': False,\n",
       " 'not bad': True,\n",
       " 'not happy': False,\n",
       " 'not sad': True,\n",
       " 'very good': True,\n",
       " 'very bad': False,\n",
       " 'very happy': True,\n",
       " 'very sad': False,\n",
       " 'i am happy': True,\n",
       " 'this is good': True,\n",
       " 'i am bad': False,\n",
       " 'this is bad': False,\n",
       " 'i am sad': False,\n",
       " 'this is sad': False,\n",
       " 'i am not happy': False,\n",
       " 'this is not good': False,\n",
       " 'i am not bad': True,\n",
       " 'this is not sad': True,\n",
       " 'i am very happy': True,\n",
       " 'this is very good': True,\n",
       " 'i am very bad': False,\n",
       " 'this is very sad': False,\n",
       " 'this is very happy': True,\n",
       " 'i am good not bad': True,\n",
       " 'this is good not bad': True,\n",
       " 'i am bad not good': False,\n",
       " 'i am good and happy': True,\n",
       " 'this is not good and not happy': False,\n",
       " 'i am not at all good': False,\n",
       " 'i am not at all bad': True,\n",
       " 'i am not at all happy': False,\n",
       " 'this is not at all sad': True,\n",
       " 'this is not at all happy': False,\n",
       " 'i am good right now': True,\n",
       " 'i am bad right now': False,\n",
       " 'this is bad right now': False,\n",
       " 'i am sad right now': False,\n",
       " 'i was good earlier': True,\n",
       " 'i was happy earlier': True,\n",
       " 'i was bad earlier': False,\n",
       " 'i was sad earlier': False,\n",
       " 'i am very bad right now': False,\n",
       " 'this is very good right now': True,\n",
       " 'this is very sad right now': False,\n",
       " 'this was bad earlier': False,\n",
       " 'this was very good earlier': True,\n",
       " 'this was very bad earlier': False,\n",
       " 'this was very happy earlier': True,\n",
       " 'this was very sad earlier': False,\n",
       " 'i was good and not bad earlier': True,\n",
       " 'i was not good and not happy earlier': False,\n",
       " 'i am not at all bad or sad right now': True,\n",
       " 'i am not at all good or happy right now': False,\n",
       " 'this was not happy and not good earlier': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c49e3b34-e7e9-4953-9872-ba424253d006",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Build a map of idx to word in the vocab\n",
    "map_idx_to_vocab = {x[0]: x[1] for x in list(enumerate(vocabulary))}\n",
    "\n",
    "# Build a map of word in the vocab to idx\n",
    "map_vocab_to_idx = {x[1]: x[0] for x in list(enumerate(vocabulary))}\n",
    "\n",
    "# Create one-hot encodings for these 18 features based on the input training data\n",
    "feature_matrix = np.zeros(shape=(len(train_data), len(vocabulary)))\n",
    "assert feature_matrix.shape == (58, 18) \n",
    "# 58 training data samples, and 18 distinct words in vocab - i.e. 18 features per sample\n",
    "\n",
    "# Consistently sized feature matrix - not sure if this is OK for RNNs?\n",
    "train_data_l = list(train_data.keys())\n",
    "for _iter in range(len(train_data)):\n",
    "    for _elem in train_data_l[_iter].split(' '):\n",
    "        feature_matrix[_iter][map_vocab_to_idx.get(_elem)] = 1\n",
    "        \n",
    "if True:\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    print(feature_matrix)\n",
    "\n",
    "# Note: Not using this feature_matrix, as I'm not sure if it fits into the whole RNN thing:\n",
    "# * RNNs can accept inputs of different sizes\n",
    "# * RNNs can create outputs of different sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0da2d38-c7a3-476c-ad71-0380a5bc5bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0.]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]]),\n",
       " array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createInputs(text: str) -> list:\n",
    "  '''\n",
    "  Returns an array of one-hot vectors representing the words\n",
    "  in the input text string.\n",
    "  - text is a string\n",
    "  - Each one-hot vector has shape (1, len(vocabulary))\n",
    "  '''\n",
    "  inputs = []\n",
    "  for w in text.split(' '):\n",
    "    v = np.zeros((1, len(vocabulary)))\n",
    "    v[0][map_vocab_to_idx[w]] = 1\n",
    "    inputs.append(v)\n",
    "  return inputs\n",
    "\n",
    "ip = createInputs('i am very good')\n",
    "\n",
    "assert len(ip) == 4\n",
    "assert ip[0].shape == (1, len(vocabulary))\n",
    "assert np.transpose(ip[0]).shape == (len(vocabulary), 1)\n",
    "ip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba94ced-f847-48ff-ba19-91348df02528",
   "metadata": {},
   "source": [
    "## RNN Model setup\n",
    "\n",
    "### RNN explained visually\n",
    "![RNN many to 1](pngs/rnn_many_to_one.png \"RNN many to 1\")  ![RNN many to many](pngs/rnn_many_to_many.png \"RNN many to many\")\n",
    "\n",
    "### RNN forward pass\n",
    "\n",
    "#### Formulae \n",
    "![RNN Formulae](pngs/rnn_formulae.png \"RNN Formulae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348da715-5202-4890-8a60-8d99e0654561",
   "metadata": {},
   "source": [
    "#### Matrix implementation\n",
    "\n",
    "Note: The `@` operator is used to multiply numpy matrixes\n",
    "\n",
    "Analysis of the first equation:\n",
    "* ht = tanh(Wxh @ X + Whh @ ht-1 + bh)\n",
    "    * ht.shape = 64x1\n",
    "    * Wxh.shape = 64x18 (18 is the number of features in training data - 64 is the size of the hidden layer matrix - I don't know why 64 was selected)\n",
    "    * Whh.shape = 64x64 (64 is the size of the hidden layer matrix - I don't know why 64 was selected)\n",
    "    * bh.shape = 64x1\n",
    "    * X.shape = 18x1 (18 is the number of features in the training data)\n",
    "\n",
    "\n",
    "\n",
    "Analysis of the second equation:\n",
    "* yt = Why @ ht + by\n",
    "    * yt.shape = 2x1 (2 was chosen in the reference url - I don't know what's the significance)\n",
    "    * Why.shape = 2x64\n",
    "    * ht.shape = 64x1\n",
    "    * by.shape = 2x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4e48e91f-efed-4be1-b338-a08d06637d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(xs):\n",
    "  # Applies the Softmax Function to the input array.\n",
    "  return np.exp(xs) / sum(np.exp(xs))\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self,\n",
    "                 size_weights_input: int, \n",
    "                 size_weights_output: int, \n",
    "                 size_weights_hidden: int = 64,\n",
    "                 debug: bool = False\n",
    "                ):\n",
    "        self.wxh = np.random.randn(size_weights_hidden, size_weights_input) / 1000\n",
    "        self.whh = np.random.randn(size_weights_hidden, size_weights_hidden) / 1000\n",
    "        self.why = np.random.randn(size_weights_output, size_weights_hidden) / 1000\n",
    "\n",
    "        self.size_weights_input = size_weights_input\n",
    "        self.size_weights_hidden = size_weights_hidden\n",
    "        self.size_weights_output = size_weights_output\n",
    "\n",
    "        self.bh = np.zeros(shape=(size_weights_hidden, 1))\n",
    "        self.by = np.zeros(shape=(size_weights_output, 1))\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward_pass(self, inputs: list) -> np.array:\n",
    "        \"\"\"Forward pass of this RNN.\n",
    "\n",
    "        Keyword arguments:\n",
    "        inputs -- a list of np.array of shape (number_of_words, len(vocabulary))\n",
    "        \n",
    "        Returns:\n",
    "        np.array -- A numpy array of size (self.size_output, 1)\n",
    "        \"\"\"\n",
    "        ht = np.zeros(shape=(self.size_weights_hidden, 1))\n",
    "\n",
    "        # Perform each step of the RNN - on each sample Xn, i.e. each word in the input\n",
    "        for _iter, word_features in enumerate(inputs):\n",
    "            if self.debug: print(\"Computing h%s\" % _iter)\n",
    "            ht = np.tanh(self.wxh @ np.transpose(word_features) + self.whh @ ht + self.bh)\n",
    "            if self.debug: print('Ht shape: {0}'.format(ht.shape))\n",
    "            if self.debug: print(ht)\n",
    "\n",
    "        # Compute the output\n",
    "        y = self.why @ ht + self.by\n",
    "\n",
    "        return y, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eda86c61-69dc-4bf9-81a7-2edc279d1a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.94111531e-06]\n",
      " [-9.44234590e-06]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50000335],\n",
       "       [0.49999665]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tests\n",
    "\n",
    "rnn = RNN(size_weights_input=len(vocabulary), \n",
    "          size_weights_output=2,\n",
    "          size_weights_hidden=64,\n",
    "          debug=False\n",
    "         )\n",
    "inputs = createInputs('i am very good')\n",
    "out, h = rnn.forward_pass(inputs)\n",
    "\n",
    "assert out.shape == (2, 1)\n",
    "assert h.shape == (64,1)\n",
    "print(out)\n",
    "probs = softmax(out)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1a9fa23c-5437-49b9-ac98-645ba8cbe5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e0d11-08fd-4480-a9f2-1ac9412e1f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fec0121-4a4b-4bb7-a189-161517921e08",
   "metadata": {},
   "source": [
    "## What I don't understand - need to read up on\n",
    "\n",
    "Post on machine learning stack exchange to get some insight and learn further. \n",
    "\n",
    "1. [ ] Could i use a standard `n x m` matrix to train a NNet? - such as the matrix `feature_matrix`?\n",
    "2. [ ] Why are the `Whh`, `Wxh` and `Why` weights set to `64x64` matrix with each position another `64x64` normal distribution matrix? Can't we just use a regular `64x64` matrix of scalar values for the matrix multiplications?\n",
    "   * Why use `randn` to generate a normal `64x64` distribution in each elem of the matrix?\n",
    "   * Why not just use `random_normal` to generate 1 matrix of scalars?\n",
    "3. [ ] What is the significance of the `size_hidden_matrix` value of 64?\n",
    "4. [ ] What is the significance of the `size_output_matrix` value of 2?\n",
    "5. [ ] Readup on `Softmax`\n",
    "6. [ ] Readup on `Cross Entropy Loss`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience-venv",
   "language": "python",
   "name": "datascience-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
