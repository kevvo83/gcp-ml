{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624f580f-330e-4916-a5ae-6231d343cc74",
   "metadata": {},
   "source": [
    "# Intro to NNets\n",
    "\n",
    "Reference: https://victorzhou.com/blog/intro-to-neural-networks/\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Create Virtual Environment: `python3 -m venv datascience-venv`\n",
    "2. Set Virtual Environment: `source datascience-venv/bin/activate`\n",
    "3. Install JupyterLab in your Virtual Env using pip: `pip3 install jupyterlab`\n",
    "4. Install dependencies (`numpy`, `pandas`, `scikit-learn`) into the virtual environment\n",
    "   * `pip3 install pandas`, `pip3 install scikit-learn`\n",
    "5. Add your Virtual Environment as a kernel to Jupyterlab: `python3 -m ipykernel install --user --name=datascience-venv`\n",
    "6. Start JupyterLab from the virtual environment: `jupyter-lab --notebook-dir <location of your notebooks>`\n",
    "7. Make sure your set your Virtual Env's kernel in the notebook that you're using"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42ce5d5f-db80-4cb5-b7fe-2721deff76ab",
   "metadata": {},
   "source": [
    "## Neural Net being built\n",
    "\n",
    "![Neural Net being built](pngs/neural_net_intro_network.svg \"Neural Net being built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af7807c-fa50-43eb-bffe-d06e12ebb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b910848-05f9-423b-a2a4-b70fa0004142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with types\n",
    "if False:\n",
    "    l = np.array([1,2,3,4,5,6])\n",
    "    l[4:6]\n",
    "    \n",
    "    [1,2,3,4,5,6][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a4edba32-4285-4148-b0ca-5833eb472562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25,  6])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play with types\n",
    "if True:\n",
    "    testdataabcd = np.array([\n",
    "      [-2, -1],  # Alice\n",
    "      [25, 6],   # Bob\n",
    "      [17, 4],   # Charlie\n",
    "      [-15, -6], # Diana\n",
    "    ])\n",
    "testdataabcd[1]\n",
    "# testdataabcd.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ba716-cc50-4e54-adaf-79e0b2e265f1",
   "metadata": {},
   "source": [
    "## Formulaic Explanation: Backpropagation\n",
    "\n",
    "Refer to the reference URL to follow along for the derivative.\n",
    "\n",
    "![Derivatives Intuition](pngs/derivatives_eq.png \"Derivatives Intuition\")\n",
    "\n",
    "```\n",
    "# Loss Function - RMSE\n",
    "L = (1/n) * np.sum(((yactual - ypred) ** 2))\n",
    "\n",
    "# Partial derivative of Loss Function / Ypred\n",
    "dL_by_dypred = -2 * (1 - ypred) # When we take 1 sample at a time\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "sigmoid'(a) = sigmoid(a) * (1 - sigmoid(a))\n",
    "\n",
    "# Partial derivative of dypred_by_dh1\n",
    "dypred_by_dh1 = w5 * sigmoid'(w5*h1 + w6*h2 + b3)\n",
    "# dypred_by_dh1 = w5 * sigmoid(w5*h1 + w6*h2 + b3) * (1 - sigmoid(w5*h1 + w6*h2 + b3))\n",
    "\n",
    "# Partial derivative of h1 over w1\n",
    "dh1_by_dw1 = w1 * sigmoid'(w1*x1 + w2*x2 + b1)\n",
    "\n",
    "# For SGD, you'll need to compute the partial derivatives for all the weights & biases and then iterate\n",
    "## The above is just for weight w1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593ce12-1a06-4348-bb5a-438395469de9",
   "metadata": {},
   "source": [
    "## Intuition Explanation: Stochastic Gradient Descent to train the NNet\n",
    "\n",
    "\n",
    "![SGD](pngs/stochastic_grad_descent_intro.png \"SGD Intro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a5570acf-81ac-49b0-9a8d-0004d8056d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(logit: float) -> float:\n",
    "    return 1 / (1 + np.exp(-1 * logit))\n",
    "\n",
    "def derivative_of_sigmoid(logit: float) -> float:\n",
    "    return sigmoid(logit) * (1 - sigmoid(logit))\n",
    "\n",
    "# RMSE - aka Root Mean Square Error\n",
    "def rmse_impl(ypred: np.array, yactual: np.array):\n",
    "    return ((ypred - yactual) ** 2).mean()\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias, activation_function):\n",
    "        self.weights: np.array = weights\n",
    "        self.bias: int = bias\n",
    "        self.activation_function = activation_function\n",
    "    def feed_forward(self, input_vector: np.array) -> float:\n",
    "        # note: Dot Product of matrixes returns a scalar value\n",
    "        return self.activation_function(np.dot(input_vector, self.weights) + self.bias)\n",
    "    def feed_forward_pre_activation(self, input_vector: np.array) -> float:\n",
    "        return (np.dot(input_vector, self.weights) + self.bias)\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, weights: list, biases: list):\n",
    "        self.weights_w1_w2: np.array = np.array(weights[0:2])\n",
    "        self.weights_w3_w4: np.array = np.array(weights[2:4])\n",
    "        self.weights_w5_w6: np.array = np.array(weights[4:6])\n",
    "        self.bias_h1 = np.array(biases[0])\n",
    "        self.bias_h2 = np.array(biases[1])\n",
    "        self.bias_h3 = np.array(biases[2])\n",
    "\n",
    "        # hidden layer\n",
    "        self.h1: Neuron = Neuron(self.weights_w1_w2, self.bias_h1, sigmoid)\n",
    "        self.h2: Neuron = Neuron(self.weights_w3_w4, self.bias_h2, sigmoid)\n",
    "\n",
    "        # output layer\n",
    "        self.o1: Neuron = Neuron(self.weights_w5_w6, self.bias_h3, sigmoid)\n",
    "\n",
    "    def feed_forward(self, input_vector: np.array) -> float:\n",
    "        # Feed the activated return values from h1 and h2 into o1\n",
    "        return self.o1.feed_forward(\n",
    "            np.array([\n",
    "                self.h1.feed_forward(input_vector),\n",
    "                self.h2.feed_forward(input_vector)\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    def train(self, in_train_X: np.array, in_train_Y: np.array, learn_rate: float, epochs: int):\n",
    "\n",
    "        result = ()\n",
    "\n",
    "        _num_samples = in_train_X.shape[0]\n",
    "        \n",
    "        for _epoch in range(epochs):\n",
    "            for _iter in range(_num_samples):\n",
    "                \n",
    "                train_X: np.array = in_train_X[_iter]\n",
    "                y_actual = in_train_Y[_iter]\n",
    "                \n",
    "                ypred = self.feed_forward(train_X)\n",
    "    \n",
    "                # Calculate partial derivatives\n",
    "                dL_by_dypred = -2 * (y_actual - ypred)\n",
    "    \n",
    "                # neurons\n",
    "                h1_pre_activation = self.h1.feed_forward_pre_activation(train_X)\n",
    "                h2_pre_activation = self.h2.feed_forward_pre_activation(train_X)\n",
    "                o1_pre_activation = self.o1.feed_forward_pre_activation(\n",
    "                    np.array([\n",
    "                        self.h1.feed_forward(train_X),\n",
    "                        self.h2.feed_forward(train_X)\n",
    "                    ])\n",
    "                )\n",
    "    \n",
    "                # dypred_by_dh1 = w5 * sigmoid'(w5*h1 + w6*h2 + b3)\n",
    "                dypred_by_dh1 = self.weights_w5_w6[0] * derivative_of_sigmoid(o1_pre_activation)\n",
    "                dypred_by_dh2 = self.weights_w5_w6[1] * derivative_of_sigmoid(o1_pre_activation)\n",
    "    \n",
    "                # dypred_by_w5 = h1 * sigmoid'(w5*h1 + w6*h2 + b3)\n",
    "                dypred_by_w5 = self.h1.feed_forward(train_X) * derivative_of_sigmoid(o1_pre_activation)\n",
    "                dypred_by_w6 = self.h2.feed_forward(train_X) * derivative_of_sigmoid(o1_pre_activation)\n",
    "                dypred_by_b3 = 1 * derivative_of_sigmoid(o1_pre_activation)\n",
    "    \n",
    "                # dh1_by_dw1 = w1 * sigmoid'(w1*x1 + w2*x2 + b1)\n",
    "                dh1_by_dw1 = self.weights_w1_w2[0] * derivative_of_sigmoid(h1_pre_activation)\n",
    "                dh1_by_dw2 = self.weights_w1_w2[1] * derivative_of_sigmoid(h1_pre_activation)\n",
    "                dh1_by_db1 = 1 * derivative_of_sigmoid(h1_pre_activation)\n",
    "    \n",
    "                dh2_by_dw3 = self.weights_w3_w4[0] * derivative_of_sigmoid(h2_pre_activation)\n",
    "                dh2_by_dw4 = self.weights_w3_w4[1] * derivative_of_sigmoid(h2_pre_activation)\n",
    "                dh2_by_db2 = 1 * derivative_of_sigmoid(h2_pre_activation)\n",
    "    \n",
    "                do1_by_dw5 = self.weights_w5_w6[0] * derivative_of_sigmoid(o1_pre_activation)\n",
    "                do1_by_dw6 = self.weights_w5_w6[1] * derivative_of_sigmoid(o1_pre_activation)\n",
    "                do1_by_db3 = 1 * derivative_of_sigmoid(o1_pre_activation)\n",
    "                \n",
    "    \n",
    "                # Update the weights to be used in the next gradient descent cycle\n",
    "                # w1, w2, b1\n",
    "                self.weights_w1_w2[0] -= learn_rate * dL_by_dypred * dypred_by_dh1 * dh1_by_dw1\n",
    "                self.weights_w1_w2[1] -= learn_rate * dL_by_dypred * dypred_by_dh1 * dh1_by_dw2\n",
    "                self.bias_h1 -= learn_rate * dL_by_dypred * dypred_by_dh1 * dh1_by_db1\n",
    "    \n",
    "                # w3, w4, b2\n",
    "                self.weights_w3_w4[0] -= learn_rate * dL_by_dypred * dypred_by_dh2 * dh2_by_dw3\n",
    "                self.weights_w3_w4[1] -= learn_rate * dL_by_dypred * dypred_by_dh2 * dh2_by_dw4\n",
    "                self.bias_h2 -= learn_rate * dL_by_dypred * dypred_by_dh2 * dh2_by_db2\n",
    "    \n",
    "                # w5, w6, b3\n",
    "                self.weights_w5_w6[0] -= learn_rate * dL_by_dypred * dypred_by_w5\n",
    "                self.weights_w5_w6[1] -= learn_rate * dL_by_dypred * dypred_by_w6\n",
    "                self.bias_h3 -= learn_rate * dL_by_dypred * dypred_by_b3\n",
    "                \n",
    "            # --- Calculate total loss at the end of each epoch\n",
    "            if _epoch % 10 == 0:\n",
    "              y_preds = np.apply_along_axis(self.feed_forward, 1, in_train_X)\n",
    "              loss = rmse_impl(in_train_Y, y_preds)\n",
    "              print(\"Epoch %d loss: %.3f\" % (_epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23f3b7f3-05f7-45dc-a1ce-48b923e25a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests \n",
    "\n",
    "# neuron methods test\n",
    "weights = np.array([0, 1]) # w1, w2 in the diagram above\n",
    "bias = 4 # b1 in the diagram above\n",
    "X = np.array([2, 3]) # input vector - weight, height in the diagram above\n",
    "\n",
    "n = Neuron(weights=weights, bias=bias, activation_function=sigmoid)\n",
    "h1 = n.feed_forward(X)\n",
    "\n",
    "assert isinstance(h1, float), \"Feed forward type is not float\"\n",
    "h1 # Expected value is meant to look like - 0.9990889488055994\n",
    "\n",
    "# neural net methods test\n",
    "nnet = NeuralNet([0, 1, 0, 1, 0, 1], [0, 0, 0])\n",
    "o1 = nnet.feed_forward(X)\n",
    "\n",
    "assert o1 == 0.7216325609518421 # Got the value from the reference link for assert comparison\n",
    "\n",
    "\n",
    "# rmse function test\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "\n",
    "rmse = rmse_impl(y_true, y_pred)\n",
    "rmse\n",
    "\n",
    "# test that the feed_forward_pre_activation method works correctly\n",
    "h1_prior_to_activation = n.feed_forward_pre_activation(X)\n",
    "assert(\n",
    "    np.dot(weights, X) + bias == h1_prior_to_activation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b5f8b2c6-43a7-4a1c-99f4-01754b6dba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.287\n",
      "Epoch 10 loss: 0.198\n",
      "Epoch 20 loss: 0.155\n",
      "Epoch 30 loss: 0.126\n",
      "Epoch 40 loss: 0.105\n",
      "Epoch 50 loss: 0.088\n",
      "Epoch 60 loss: 0.075\n",
      "Epoch 70 loss: 0.064\n",
      "Epoch 80 loss: 0.056\n",
      "Epoch 90 loss: 0.049\n",
      "Epoch 100 loss: 0.043\n",
      "Epoch 110 loss: 0.039\n",
      "Epoch 120 loss: 0.035\n",
      "Epoch 130 loss: 0.031\n",
      "Epoch 140 loss: 0.029\n",
      "Epoch 150 loss: 0.026\n",
      "Epoch 160 loss: 0.024\n",
      "Epoch 170 loss: 0.022\n",
      "Epoch 180 loss: 0.021\n",
      "Epoch 190 loss: 0.019\n",
      "Epoch 200 loss: 0.018\n",
      "Epoch 210 loss: 0.017\n",
      "Epoch 220 loss: 0.016\n",
      "Epoch 230 loss: 0.015\n",
      "Epoch 240 loss: 0.014\n",
      "Epoch 250 loss: 0.013\n",
      "Epoch 260 loss: 0.013\n",
      "Epoch 270 loss: 0.012\n",
      "Epoch 280 loss: 0.012\n",
      "Epoch 290 loss: 0.011\n",
      "Epoch 300 loss: 0.011\n",
      "Epoch 310 loss: 0.010\n",
      "Epoch 320 loss: 0.010\n",
      "Epoch 330 loss: 0.009\n",
      "Epoch 340 loss: 0.009\n",
      "Epoch 350 loss: 0.009\n",
      "Epoch 360 loss: 0.008\n",
      "Epoch 370 loss: 0.008\n",
      "Epoch 380 loss: 0.008\n",
      "Epoch 390 loss: 0.008\n",
      "Epoch 400 loss: 0.007\n",
      "Epoch 410 loss: 0.007\n",
      "Epoch 420 loss: 0.007\n",
      "Epoch 430 loss: 0.007\n",
      "Epoch 440 loss: 0.007\n",
      "Epoch 450 loss: 0.006\n",
      "Epoch 460 loss: 0.006\n",
      "Epoch 470 loss: 0.006\n",
      "Epoch 480 loss: 0.006\n",
      "Epoch 490 loss: 0.006\n",
      "Epoch 500 loss: 0.006\n",
      "Epoch 510 loss: 0.005\n",
      "Epoch 520 loss: 0.005\n",
      "Epoch 530 loss: 0.005\n",
      "Epoch 540 loss: 0.005\n",
      "Epoch 550 loss: 0.005\n",
      "Epoch 560 loss: 0.005\n",
      "Epoch 570 loss: 0.005\n",
      "Epoch 580 loss: 0.005\n",
      "Epoch 590 loss: 0.005\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 610 loss: 0.004\n",
      "Epoch 620 loss: 0.004\n",
      "Epoch 630 loss: 0.004\n",
      "Epoch 640 loss: 0.004\n",
      "Epoch 650 loss: 0.004\n",
      "Epoch 660 loss: 0.004\n",
      "Epoch 670 loss: 0.004\n",
      "Epoch 680 loss: 0.004\n",
      "Epoch 690 loss: 0.004\n",
      "Epoch 700 loss: 0.004\n",
      "Epoch 710 loss: 0.004\n",
      "Epoch 720 loss: 0.004\n",
      "Epoch 730 loss: 0.004\n",
      "Epoch 740 loss: 0.003\n",
      "Epoch 750 loss: 0.003\n",
      "Epoch 760 loss: 0.003\n",
      "Epoch 770 loss: 0.003\n",
      "Epoch 780 loss: 0.003\n",
      "Epoch 790 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 810 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 830 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 870 loss: 0.003\n",
      "Epoch 880 loss: 0.003\n",
      "Epoch 890 loss: 0.003\n",
      "Epoch 900 loss: 0.003\n",
      "Epoch 910 loss: 0.003\n",
      "Epoch 920 loss: 0.003\n",
      "Epoch 930 loss: 0.003\n",
      "Epoch 940 loss: 0.003\n",
      "Epoch 950 loss: 0.003\n",
      "Epoch 960 loss: 0.003\n",
      "Epoch 970 loss: 0.003\n",
      "Epoch 980 loss: 0.003\n",
      "Epoch 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Labelled training dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = NeuralNet(\n",
    "    np.random.normal(size=(1,6)).tolist()[0], # weights\n",
    "    np.random.normal(size=(1,3)).tolist()[0] # biases\n",
    ")\n",
    "network.train(data, all_y_trues, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e0debbbc-55f2-4b29-9503-4d09172ad2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.972\n",
      "Frank: 0.638\n",
      "Frank2: 0.072\n",
      "Training data: Alice: 0.950\n",
      "Training data: Bob: 0.056\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions - based on the model trained in the previous cell\n",
    "\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches - expected prediction is: 0.951 - F\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches - expected prediction is: 0.039 - M\n",
    "frank2 = np.array([20, 4])\n",
    "print(\"Emily: %.3f\" % network.feed_forward(emily))\n",
    "print(\"Frank: %.3f\" % network.feed_forward(frank))\n",
    "print(\"Frank2: %.3f\" % network.feed_forward(frank2))\n",
    "\n",
    "print(\"Training data: Alice: %.3f\" % network.feed_forward(np.array([-2, -1])))\n",
    "print(\"Training data: Bob: %.3f\" % network.feed_forward(np.array([25, 6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0dca3-02f2-410c-b072-286330e54b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## END RESULT\n",
    "The model doesn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience-venv",
   "language": "python",
   "name": "datascience-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
